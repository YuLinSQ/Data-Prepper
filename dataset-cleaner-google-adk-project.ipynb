{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Note: honestly, I was pretty tempted to create a model that preps data based on type of data and model it will feed into,\n# but opted to choose this simplified generalized version due to time constraints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:16:51.294008Z","iopub.execute_input":"2025-12-01T04:16:51.294366Z","iopub.status.idle":"2025-12-01T04:16:51.299295Z","shell.execute_reply.started":"2025-12-01T04:16:51.294322Z","shell.execute_reply":"2025-12-01T04:16:51.298309Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# try: response block to code, combine code from each agent and turn it into a code block,\n# then execute out with exec(df) - remember to global df too\n# smth like this: context = {'df': df} \n# exec(agent_code, context)\n# df_modified = context['df']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:16:51.302037Z","iopub.execute_input":"2025-12-01T04:16:51.302344Z","iopub.status.idle":"2025-12-01T04:16:51.320573Z","shell.execute_reply.started":"2025-12-01T04:16:51.302323Z","shell.execute_reply":"2025-12-01T04:16:51.319661Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# here is the summary of agents used and in order:\n# Standardize: Fix types, remove currency symbols, unify text casing.\n# Date: Extract features (Year/Month) from timestamps.\n# Duplicates: Remove exact row matches (Safety: <1%).\n# Grouper: Group rare categories into \"Other\" (Safety: <1%).\n# Nulls: Drop bad cols/rows or Impute (Median/Mode).\n# Correlation: Drop redundant features (Correlation > 95%).\n# Skew: Log-transform positive outliers.\n# Encoding: Convert categories to numbers (One-Hot vs Label).\n# Scaler: Split Train/Test and Standardize (Prevent Leakage).\n# Bias: Fixes Statistical Bias and Class Imbalance on Train set.\n# Mastermind: orchestrates the code of these 10 agents to work in sequential order on a given dataset.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:16:51.321571Z","iopub.execute_input":"2025-12-01T04:16:51.322009Z","iopub.status.idle":"2025-12-01T04:16:51.340190Z","shell.execute_reply.started":"2025-12-01T04:16:51.321985Z","shell.execute_reply":"2025-12-01T04:16:51.339196Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# setup keys\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"‚úÖ Setup and authentication complete.\")\nexcept Exception as e:\n    print(\n        f\"üîë Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:16:51.341297Z","iopub.execute_input":"2025-12-01T04:16:51.341685Z","iopub.status.idle":"2025-12-01T04:16:51.540504Z","shell.execute_reply.started":"2025-12-01T04:16:51.341653Z","shell.execute_reply":"2025-12-01T04:16:51.539329Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Setup and authentication complete.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# import libraries\nfrom google.genai import types\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import google_search, AgentTool, ToolContext\nfrom google.adk.code_executors import BuiltInCodeExecutor\nfrom google.adk.code_executors import UnsafeLocalCodeExecutor\n\nprint(\"‚úÖ ADK components imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:16:51.541594Z","iopub.execute_input":"2025-12-01T04:16:51.541908Z","iopub.status.idle":"2025-12-01T04:17:46.882786Z","shell.execute_reply.started":"2025-12-01T04:16:51.541880Z","shell.execute_reply":"2025-12-01T04:17:46.881902Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ADK components imported successfully.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# configure retry options\nretry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:46.883786Z","iopub.execute_input":"2025-12-01T04:17:46.885144Z","iopub.status.idle":"2025-12-01T04:17:46.890107Z","shell.execute_reply.started":"2025-12-01T04:17:46.885117Z","shell.execute_reply":"2025-12-01T04:17:46.889227Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\n# using a randomized dataset, in production replace with an actual dataset\n# Set seed for reproducibility\nnp.random.seed(2)\n\ndef generate_messy_dataset(rows=1000):\n    print(\"‚ö†Ô∏è Generating The Doomed Dataset...\")\n    \n    # 1. BASE DATA & SKEW (Triggers SkewAgent)\n    # Generate a log-normal distribution (Right skewed)\n    transaction_amt = np.random.lognormal(mean=2, sigma=1, size=rows)\n    \n    # 2. REDUNDANT FEATURES (Triggers CorrelationAgent)\n    # Celsius and Fahrenheit are perfectly correlated\n    temp_c = np.random.normal(25, 5, rows)\n    temp_f = temp_c * 9/5 + 32\n    \n    # 3. MESSY STRINGS & CURRENCY (Triggers StandardizeAgent)\n    # Includes whitespace, different cases, and symbols\n    cities = [\"  new york \", \"New York\", \"SF\", \"sf \", \"chicago\", \"Chicago\", \"  Austin\"]\n    city_col = np.random.choice(cities, rows)\n    \n    salaries = np.random.randint(40000, 150000, rows).astype(str)\n    # Corrupt 30% of salaries with currency symbols\n    for i in range(rows):\n        if np.random.rand() < 0.3:\n            salaries[i] = f\"${salaries[i]}\"\n        if np.random.rand() < 0.1:\n            salaries[i] = f\"{salaries[i]},00\" # European style comma/decimal mix\n            \n    # 4. DATES (Triggers DateAgent)\n    # Mix of formats and NaTs\n    start_date = pd.to_datetime('2020-01-01')\n    dates = [start_date + pd.Timedelta(days=x) for x in range(rows)]\n    date_strings = [d.strftime('%Y-%m-%d') for d in dates]\n    # Corrupt some dates\n    date_strings[0] = \"Not a Date\"\n    date_strings[10] = \"Unknown\"\n    \n    # 5. NULLS & MISSING DATA (Triggers NullAgent)\n    # A. > 50% Missing (Should be dropped entirely)\n    mostly_empty = np.array([np.nan] * rows)\n    mostly_empty[:10] = 1 # Only 10 values exist\n    \n    # B. < 5% Missing (Rows should be dropped)\n    tiny_missing = np.random.rand(rows)\n    tiny_missing[:15] = np.nan # 1.5% missing\n    \n    # C. ~20% Missing (Should be Imputed)\n    medium_missing_age = np.random.randint(18, 70, rows).astype(float)\n    medium_missing_age[:200] = np.nan # 20% missing\n    \n    # 6. RARE CATEGORIES (Triggers GrouperAgent)\n    # 'Google' and 'Direct' are common; 'Friend' and 'Billboard' are rare (<1%)\n    sources = ['Google']*800 + ['Direct']*190 + ['Friend']*5 + ['Billboard']*5\n    np.random.shuffle(sources)\n    \n    # 7. HIGH CARDINALITY (Triggers EncodingAgent - Label Encode)\n    # 50 unique ZIP codes\n    zips = np.random.randint(90000, 90050, rows).astype(str)\n    \n    # 8. LOW CARDINALITY (Triggers EncodingAgent - One-Hot Encode)\n    membership = np.random.choice(['Gold', 'Silver', 'Bronze'], rows)\n    \n    # 9. CLASS IMBALANCE (Triggers AutoBalanceAgent)\n    # 90% Class 0, 10% Class 1\n    target = np.random.choice([0, 1], rows, p=[0.90, 0.10])\n    \n    # CREATE DATAFRAME\n    df = pd.DataFrame({\n        'ID_Column': range(rows), # Should be ignored by Skew/Scaling agents\n        'Transaction_Amt': transaction_amt, # Skewed\n        'Temp_C': temp_c, # Redundant\n        'Temp_F': temp_f, # Redundant to be dropped\n        'City': city_col, # Messy text\n        'Salary': salaries, # Messy numbers ($)\n        'Join_Date': date_strings, # Date parsing\n        'Garbage_Col': mostly_empty, # >50% null\n        'Sensor_Reading': tiny_missing, # <5% null\n        'User_Age': medium_missing_age, # Impute median\n        'Referral': sources, # Group 'Friend' -> Other\n        'Zip_Code': zips, # Label Encode\n        'Membership': membership, # One-Hot Encode\n        'Target_Label': target # Imbalanced\n    })\n    \n    # 10. DUPLICATES (Triggers DuplicatesAgent)\n    # Append top 5 rows to bottom to create exact duplicates (0.5% duplicates)\n    df = pd.concat([df, df.head(5)], ignore_index=True)\n    \n    print(\"‚úÖ Dataset Created. Shape:\", df.shape)\n    return df\n\n# Initialize\ndf = generate_messy_dataset()\n\n# Save to the bridge location\ndf.to_pickle('/kaggle/working/df.pkl')\nprint(\"‚úÖ SETUP: Messy dataframe created at '/kaggle/working/df.pkl'\")\nprint(f\"Original Columns: {df.columns.tolist()}\")\nprint(\"-\" * 30)\ndf = pd.read_pickle('/kaggle/working/df.pkl')\nprint(\"Columns:\", df.columns.tolist())\nprint(df.head())\nlocal_executor = UnsafeLocalCodeExecutor(working_dir='/kaggle/working')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:46.893641Z","iopub.execute_input":"2025-12-01T04:17:46.894005Z","iopub.status.idle":"2025-12-01T04:17:46.966374Z","shell.execute_reply.started":"2025-12-01T04:17:46.893974Z","shell.execute_reply":"2025-12-01T04:17:46.965291Z"}},"outputs":[{"name":"stdout","text":"‚ö†Ô∏è Generating The Doomed Dataset...\n‚úÖ Dataset Created. Shape: (1005, 14)\n‚úÖ SETUP: Messy dataframe created at '/kaggle/working/df.pkl'\nOriginal Columns: ['ID_Column', 'Transaction_Amt', 'Temp_C', 'Temp_F', 'City', 'Salary', 'Join_Date', 'Garbage_Col', 'Sensor_Reading', 'User_Age', 'Referral', 'Zip_Code', 'Membership', 'Target_Label']\n------------------------------\nColumns: ['ID_Column', 'Transaction_Amt', 'Temp_C', 'Temp_F', 'City', 'Salary', 'Join_Date', 'Garbage_Col', 'Sensor_Reading', 'User_Age', 'Referral', 'Zip_Code', 'Membership', 'Target_Label']\n   ID_Column  Transaction_Amt     Temp_C     Temp_F      City  Salary  \\\n0          0         4.870722  25.988750  78.779749        SF   91833   \n1          1         6.984778  26.684181  80.031525  New York  $57890   \n2          2         0.872671  22.011714  71.621085   chicago  111204   \n3          3        38.102154  26.689683  80.041429    Austin   82824   \n4          4         1.229447  21.349859  70.429747       sf   124235   \n\n    Join_Date  Garbage_Col  Sensor_Reading  User_Age Referral Zip_Code  \\\n0  Not a Date          1.0             NaN       NaN   Direct    90012   \n1  2020-01-02          1.0             NaN       NaN   Google    90002   \n2  2020-01-03          1.0             NaN       NaN   Google    90004   \n3  2020-01-04          1.0             NaN       NaN   Direct    90025   \n4  2020-01-05          1.0             NaN       NaN   Google    90043   \n\n  Membership  Target_Label  \n0     Silver             0  \n1     Bronze             0  \n2     Bronze             0  \n3     Bronze             0  \n4     Bronze             0  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Note: have to output store df, train_df, and test_df safe path\n# because the agent framework's execution sandbox does not persist local variable assignments or\n# global scope modifications back to the main environment after the agent finishes.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:46.967350Z","iopub.execute_input":"2025-12-01T04:17:46.967634Z","iopub.status.idle":"2025-12-01T04:17:46.971761Z","shell.execute_reply.started":"2025-12-01T04:17:46.967603Z","shell.execute_reply":"2025-12-01T04:17:46.970832Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# standardize text based on appearance percentage and similarity, strings to number if possible\nstandardize_agent = LlmAgent(\n    name=\"StandardizeAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert.\n    \n    **YOUR GOAL:**\n    Write a generic, robust Python script to clean and standardize ANY dataset without knowing column names in advance.\n\n    **CRITICAL SETUP:**\n    1. Imports: `import pandas as pd; import numpy as np; import warnings; warnings.filterwarnings('ignore')`\n    2. Load: `df = pd.read_pickle('/kaggle/working/df.pkl')`\n\n    **ADAPTIVE TRANSFORMATION LOGIC:**\n    \n    1. **Step 1: Clean Headers**\n       - Strip whitespace, lowercase, and replace spaces with underscores for all column names.\n\n    2. **Step 2: Smart Type Inference (Iterate through OBJECT columns only)**\n       For each column where `dtype == 'object'`, perform the following checks IN ORDER:\n\n       * **A. Check for Numeric/Currency:**\n           - Create a temporary clean version: Remove '$', ',', and whitespace.\n           - **DO NOT REMOVE '.' (Decimal Points) or '-' (Negative Signs).**\n             - *Hint:* Use regex `r'[$,]'` to remove only specific symbols, NOT `r'[^\\d]'`.\n           - Attempt convert: `temp = pd.to_numeric(clean_col, errors='coerce')`\n           - **The Safety Check:** Calculate the ratio of Non-Null values in `temp` vs the original column.\n           - **Decision:** IF `temp.notna().mean()` > 0.8 (meaning >80% of data successfully converted):\n             - Apply the conversion to the actual column.\n             - Continue to next column (do not check Date).\n\n       * **B. Check for Date/Time:**\n           - Attempt convert: `temp = pd.to_datetime(col, errors='coerce')`\n           - **The Safety Check:** Calculate the ratio of Non-Null values.\n           - **Decision:** IF `temp.notna().mean()` >= 0.6 (meaning >60% is a valid date):\n             - Apply the conversion.\n             - Continue to next column.\n\n       * **C. Fallback: Text Cleaning:**\n           - If neither Numeric nor Date checks pass:\n             - Strip whitespace: `df[col] = df[col].astype(str).str.strip()`\n             - Title case: `df[col] = df[col].str.title()`\n             - Replace empty strings with `np.nan`.\n\n    **OUTPUT RULES:**\n    - Output the Python code block to perform the transformation.\n    - End with: `print(df.dtypes); df.to_pickle('/kaggle/working/df.pkl'); print(\"Standardized.\")`\n    - After the code executes successfully (and prints the result), you MUST respond with the text: \"Standardization complete.\"\n    \"\"\",\n    code_executor=local_executor,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:46.972850Z","iopub.execute_input":"2025-12-01T04:17:46.973169Z","iopub.status.idle":"2025-12-01T04:17:46.992837Z","shell.execute_reply.started":"2025-12-01T04:17:46.973140Z","shell.execute_reply":"2025-12-01T04:17:46.991683Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#  feature creation ex - spilt dates\nDate_Agent = LlmAgent(\n    name=\"DateAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to Create Features by splitting Date/Time columns in the variable `df`.\n\n    **CRITICAL:**\n        1. Load 'df.pkl'. Do NOT create dummy data. Extract features from 'df.pkl', overwrite 'df.pkl'.\n        2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Identify Candidates:**\n       - Iterate through all columns.\n       - Target columns where:\n         - Dtype is already `datetime`.\n         - OR Name contains: \"date\", \"time\", \"joined\", \"created\", \"at\" (case-insensitive) AND Dtype is `object`.\n    \n    2. **Safe Conversion:**\n       - For candidates, attempt: `temp = pd.to_datetime(df[col], errors='coerce')`\n       - **Validation:** Check the NaT (Null) rate of `temp`.\n         - IF NaT rate > 50%: The column is likely NOT a real date. **SKIP IT.**\n         - IF NaT rate <= 50%: Assign `df[col] = temp` and proceed to step 3.\n\n    3. **Feature Splitting (The \"Creation\" Step):**\n       - For every valid date column:\n         - Create `{col}_year`: `df[col].dt.year`\n         - Create `{col}_month`: `df[col].dt.month`\n         - Create `{col}_day`: `df[col].dt.day`\n         - Create `{col}_dow`: `df[col].dt.dayofweek` (0=Mon, 6=Sun)\n         - Create `{col}_is_weekend`: `(df[col].dt.dayofweek >= 5).astype(int)`\n\n    4. **Cleanup:**\n       - DROP the original date column after extracting features (Models cannot digest raw timestamps).\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. End with:\n        1. `print(f\"Date Features Created. New Shape: {df.shape}\")` and `print(df.columns.tolist())`.\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Date features added. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Date extraction complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:46.994159Z","iopub.execute_input":"2025-12-01T04:17:46.994498Z","iopub.status.idle":"2025-12-01T04:17:47.020259Z","shell.execute_reply.started":"2025-12-01T04:17:46.994470Z","shell.execute_reply":"2025-12-01T04:17:47.019146Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# remove duplicates data <1% for safety\nduplicates_agent = LlmAgent(\n    name=\"DuplicatesAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to remove duplicate data safely from a variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Remove duplicates from 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n    2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Calculate Duplicates:** Identify how many rows would be removed using exact row matching.\n    2. **The 1% Safety Rule:** - Calculate the drop percentage: `(duplicates_count / total_rows) * 100`.\n       - IF the drop percentage is **greater than 1%**: Do NOT modify `df`. Instead, print(f\"Aborting: Duplicates exceed 1% safety limit. Found {pct}%\")`.\n       - IF the drop percentage is **less than or equal to 1%**: Remove the duplicates permanently from `df`.\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. End with:\n        1. printing: `print(f\"Successfully dropped {dropped_count} rows. New shape: {df.shape}\")`\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Duplicates removed. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Duplicate removal complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.021264Z","iopub.execute_input":"2025-12-01T04:17:47.021534Z","iopub.status.idle":"2025-12-01T04:17:47.047159Z","shell.execute_reply.started":"2025-12-01T04:17:47.021514Z","shell.execute_reply":"2025-12-01T04:17:47.046416Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# groups rare categories into \"other\" <1%, 0.5% individually\ngrouper_agent = LlmAgent(\n    name=\"GrouperAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language.\n\n    **YOUR GOAL:**\n    **GOAL:** Group rare categories (appearing < 0.5%) into 'Other' if the total impact is < 1%.\n\n    **CRITICAL:** Load 'df.pkl'. Group rare categories from 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n    2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **LOGIC:**\n    1. Iterate through columns where `dtype == 'object'`.\n    2. For each column:\n       - Calculate value counts (normalized).\n       - Identify \"rare_values\": those with `frequency < 0.005` (0.5%).\n       - Calculate \"impact\": Sum of frequencies of all `rare_values`.\n    3. **Decision Rule:**\n       - **IF impact <= 0.01 (1%):** - Replace `rare_values` with the string \"Other\".\n         - Print: `f\"Column '{col}': Grouped {count} categories ({impact:.2%})\"`\n       - **ELSE (impact > 1%):**\n         - Print: `f\"Column '{col}': Skipped (Impact {impact:.2%} > 1% safety limit)\"`\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. The code MUST end with:\n        1. A summary print for each column: `print(f\"Column '{col}': Grouped {count} rows\")`.\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Grouping complete. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Duplicate removal complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.048034Z","iopub.execute_input":"2025-12-01T04:17:47.048394Z","iopub.status.idle":"2025-12-01T04:17:47.073349Z","shell.execute_reply.started":"2025-12-01T04:17:47.048329Z","shell.execute_reply":"2025-12-01T04:17:47.072425Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Drop rows if the missing data is minimal (<5%), columns if the feature is mostly empty (>50%)\n# Use median to fill in if possible\n# total thresholding to prevent cascading data loss\nNull_agent = LlmAgent(\n    name=\"NullAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to handle missing data (nulls).\n\n    **CRITICAL:** Load 'df.pkl'. Handle nulls in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n    2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **LOGIC - EXECUTE IN THIS ORDER:**\n    1. **Column Cleanup:**\n       - IF a column is missing > 50% data: Drop the **Column**.\n\n    2. **Total Row Safety Check:**\n       - Identify ALL rows that contain nulls in the remaining columns.\n       - Calculate `total_rows_with_nulls`.\n       - Calculate `loss_pct = (total_rows_with_nulls / total_rows) * 100`.\n\n    3. **Decision Branch:**\n       - **IF loss_pct =< 5%:**\n         - Drop ALL rows containing nulls. (Safe to drop).\n       - **ELSE (If loss_pct > 5%):**\n         - Do NOT drop rows. Instead, Impute (Fill) data:\n         - Numerics -> Fill with Median.\n         - Object/String -> Fill with Mode (Top value).\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. End with: \n        1. `print(f\"Action taken: {'Dropped Rows' if loss < 5 else 'Imputed Data'}. New Shape: {df.shape}\")`\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Redundancy removed. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Nulls handled.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.074375Z","iopub.execute_input":"2025-12-01T04:17:47.075230Z","iopub.status.idle":"2025-12-01T04:17:47.097716Z","shell.execute_reply.started":"2025-12-01T04:17:47.075182Z","shell.execute_reply":"2025-12-01T04:17:47.096955Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# PCA - correlation matrix, drops var/features with little relevance\nCorrelation_Agent = LlmAgent(\n    name=\"CorrelationAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` and `numpy` to remove **Redundant Features** (Multicollinearity) based on a correlation matrix.\n\n    **CRITICAL:** Load 'df.pkl'. Drop correlated cols in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n    2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **LOGIC:**\n    1. Select numeric columns: `nums = df.select_dtypes(include=[np.number])`\n    2. Safety Check: If `nums.shape[1] < 2`, stop and save.\n    3. **Calculate Matrix:** `corr_matrix = nums.corr().abs()`\n    4. **Select Upper Triangle (CRITICAL):** - Use this logic to avoid self-correlation:\n       - `upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))`\n    5. **Identify Drops:**\n       - Find columns where any value in `upper` is > 0.95.\n       - `to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]`\n    6. **Drop:** `df = df.drop(columns=to_drop)`\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. End with:\n        1.`print(f\"Dropped {len(to_drop)} redundant features: {to_drop}. New Shape: {df.shape}\")`\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Redundancy removed. Saved to df.pkl\")\n    3. Use ONLY standard libraries, `numpy`, and `pandas`.\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"PCA complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.098573Z","iopub.execute_input":"2025-12-01T04:17:47.098816Z","iopub.status.idle":"2025-12-01T04:17:47.123225Z","shell.execute_reply.started":"2025-12-01T04:17:47.098796Z","shell.execute_reply":"2025-12-01T04:17:47.122298Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# for numeric values, if skew() > 1, log transform for outliers (log(x+1)) if no neg values\n# exclude ID rows and dates\nSkew_Agent = LlmAgent(\n    name=\"SkewAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` and `numpy` to fix positive skewness in the variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Fix skew in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n    2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **LOGIC:**\n    1. Iterate through **Numeric Columns** (float/int).\n    2. **Exclusion Check:** - SKIP if name contains 'id', 'ID', 'Id'.\n       - SKIP if name ends with '_year', '_month', '_day', '_dow', '_weekend'.\n    3. **Value Check:**\n       - SKIP if column contains negative values (Log is undefined).\n    4. **Transformation:**\n       - Calculate `old_skew = df[col].skew()`.\n       - **IF old_skew > 1:**\n         - Apply: `df[col] = np.log1p(df[col])`\n         - Recalculate: `new_skew = df[col].skew()`\n         - Print: `f\"Column '{col}': Skew fixed ({old_skew:.2f} -> {new_skew:.2f})\"`\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    3. The code MUST end with:\n        1. A summary loop printing: `print(f\"Column '{col}': Skew from {old_skew} -> {new_skew}\")` for changed columns only.\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Skew fixed. Saved to df.pkl\")\n    4. Use ONLY standard libraries, `numpy`, and `pandas`.\n    5. After the code executes successfully (and prints the result), you MUST respond with the text: \"Skew fix complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.124339Z","iopub.execute_input":"2025-12-01T04:17:47.124620Z","iopub.status.idle":"2025-12-01T04:17:47.150435Z","shell.execute_reply.started":"2025-12-01T04:17:47.124592Z","shell.execute_reply":"2025-12-01T04:17:47.149508Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# one hot encoding for low cardinality\n# label encoding for high and ordinal data\nEncoding_Agent = LlmAgent(\n    name=\"EncodingAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to encode categorical data (One-Hot vs Label Encoding) in the variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Encode 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n    2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **LOGIC:**\n    1. **Identify Object Columns:** `obj_cols = df.select_dtypes(include=['object']).columns`\n    2. **Filter Exclusions:** Remove columns containing 'id', 'ID', or 'Id'.\n    \n    3. **Strategy Separation:**\n       - Create two lists: `one_hot_cols` and `label_cols`.\n       - Iterate through candidates:\n         - IF `df[col].nunique() < 10`: Add to `one_hot_cols`.\n         - ELSE: Add to `label_cols`.\n\n    4. **Execution:**\n       - **Step A (Label Encoding):**\n         - For col in `label_cols`: `df[col] = df[col].astype('category').cat.codes`\n       \n       - **Step B (One-Hot Encoding):**\n         - IF `one_hot_cols` is not empty:\n           - `df = pd.get_dummies(df, columns=one_hot_cols, prefix=one_hot_cols, dtype=int)`\n           - *Note:* This automatically drops original columns and appends new ones as Integers (0/1).\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. The code MUST end with:\n        1. `print(f\"Encoding Complete. New Shape: {df.shape}\")` and `print(df.dtypes)`.\n        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Encoding complete. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Data Encoding complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.151408Z","iopub.execute_input":"2025-12-01T04:17:47.151700Z","iopub.status.idle":"2025-12-01T04:17:47.174492Z","shell.execute_reply.started":"2025-12-01T04:17:47.151674Z","shell.execute_reply":"2025-12-01T04:17:47.173337Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# feature engineering - z score standardization for reducing scale\n# ^ spilt data train and test, prevents leakage, then transform test set using precalculated values (mean/std)\n# Not using normalization here since it's weaker to outliers and is better on Neural Nets/KNNs/Images\nScaler_Agent = LlmAgent(\n    name=\"ScalerAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `sklearn` to split the data and perform Z-Score Standardization while preventing Data Leakage.\n\n    **CRITICAL:**\n        1. Load 'df.pkl'. DO NOT create dummy data. Save 'train_df.pkl' and 'test_df.pkl'.\n        2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n               from sklearn.model_selection import train_test_split\n               from sklearn.preprocessing import StandardScaler\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n\n    **LOGIC:**\n        1. **Identify Target:** Look for a column named 'Target_Label', 'target', or 'label'.\n        2. **Split:** - `train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)`\n        3. **Define Scaling Features:**\n           - Identify Numeric Columns in `train_df`.\n           - **EXCLUSION:** Remove the Target column and any 'ID' columns from the list of columns to scale.\n        4. **Apply Scaler (Z-Score Standardization):**\n           - `scaler = StandardScaler()`\n           - `train_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])`\n           - `test_df[scale_cols] = scaler.transform(test_df[scale_cols])`\n           - *Note:* This prevents data leakage by using Train statistics for Test data.\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. **CRITICAL:** Use absolute paths for saving:\n        - train_df.to_pickle('/kaggle/working/train_df.pkl')\n        - test_df.to_pickle('/kaggle/working/test_df.pkl')\n        - print(\"Split & Scaled. Created train_df.pkl and test_df.pkl\")\n    3. End with:\n       - `print(f\"Split Complete. Train Shape: {train_df.shape}, Test Shape: {test_df.shape}\")`\n       - `print(\"Z-Score Standardization applied safely.\")`\n    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Data Scalar complete.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.175416Z","iopub.execute_input":"2025-12-01T04:17:47.175658Z","iopub.status.idle":"2025-12-01T04:17:47.196326Z","shell.execute_reply.started":"2025-12-01T04:17:47.175637Z","shell.execute_reply":"2025-12-01T04:17:47.195370Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Fixes Statistical Bias and Class Imbalance on Train set\n# To prevent overfitting, we introduce jittering/noise injection\n# ^ but obviously exclude target so the model doesn't predict that you have 1.2 siblings or smth\nAutoBalance_Agent = LlmAgent(\n    name=\"AutoBalanceAgent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `sklearn`, `numpy`, and `pandas` to Balance Training Data safely using **Noise Injection** (Jittering).\n\n    **CRITICAL:**\n        1. Load 'train_df.pkl'. DO NOT create dummy data. Balance 'train_df.pkl' and overwrite 'train_df.pkl'.\n        2. Start your code with:\n               import warnings\n               warnings.filterwarnings('ignore')\n\n    **INPUT CONTEXT:**\n    1. load `train_df` by: train_df = pd.read_pickle('/kaggle/working/train_df.pkl')\n\n    **LOGIC:**\n    1. **Auto-Detect Target:**\n       - Look for column names: [\"target\", \"label\", \"class\", \"outcome\", \"y\"]. Use first match.\n       - Else, use the last column. Save as `target_col`.\n\n    2. **Safety Checks:**\n       - **Regression Check:** If `train_df[target_col].nunique() > 20`: \n         - Print \"Regression detected (High Cardinality). Skipping balance.\"\n         - Save and Exit.\n       - **Balance Check:** Calculate `minority_count / majority_count`.\n         - If Ratio >= 0.8: Print \"Already balanced.\" -> Exit.\n\n    3. **Oversampling Strategy (The \"Synthetic Block\"):**\n       - Separate: `df_maj` (Majority) and `df_min` (Minority).\n       - Calculate needed: `n_samples = len(df_maj) - len(df_min)`.\n       - **Generate Synthetic:** `df_synthetic = resample(df_min, replace=True, n_samples=n_samples, random_state=42)`\n       \n    4. **Inject Noise (Jitter):**\n       - Iterate through **Numeric Columns** of `df_synthetic` ONLY.\n       - **Exclude:** `target_col` and any 'ID' columns.\n       - **Logic:** `df_synthetic[col] += np.random.normal(0, 0.01 * df_synthetic[col].std(), size=n_samples)`\n       - *Note:* This prevents exact duplicates.\n\n    5. **Combine:**\n       - `train_df = pd.concat([df_maj, df_min, df_synthetic])`\n\n    **OUTPUT RULES:**\n    1. Output the Python code block to perform the transformation.\n    2. End with:\n        1. train_df.to_pickle('/kaggle/working/train_df.pkl')\n        2. `print(f\"Target: '{target_col}'. Balanced with Jitter. New Shape: {train_df.shape}\")`.\n    3. After the code executes successfully (and prints the result), you MUST respond with the text: \"Bias Eliminated.\"\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=local_executor, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.197354Z","iopub.execute_input":"2025-12-01T04:17:47.197683Z","iopub.status.idle":"2025-12-01T04:17:47.220134Z","shell.execute_reply.started":"2025-12-01T04:17:47.197657Z","shell.execute_reply":"2025-12-01T04:17:47.219125Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Overall Model Orchestrator\nMastermind = LlmAgent(\n    name=\"Mastermind_agent\",\n    model=Gemini(model=\"gemini-2.0-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are an efficient Data Prepping Orchestrator.\n    \n    **YOUR MISSION:**\n    Execute the following data engineering pipeline strictly in order. \n    You do not need to write code yourself; use the provided Tools to generate and execute the code.\n    All agents must read/write from the CURRENT DIRECTORY (/kaggle/working).\n    The tools will handle file I/O automatically using fixed filenames: 'df.pkl', 'train_df.pkl', 'test_df.pkl'.\n    \n    **PIPELINE SEQUENCE:**\n    1.  **standardize_agent**: Clean formatting (df -> df).\n    2.  **Date_Agent**: Extract time features (df -> df).\n    3.  **duplicates_agent**: Remove rows (df -> df).\n    4.  **grouper_agent**: Group rare categories (df -> df).\n    5.  **Null_agent**: Impute or drop missing data (df -> df).\n    6.  **Correlation_Agent**: Drop redundant features (df -> df).\n    7.  **Skew_Agent**: Fix numeric skew (df -> df).\n    8.  **Encoding_Agent**: Categorical to Numerical (df -> df).\n    9.  **Scaler_Agent**: SPLIT into Train/Test and Scale (df -> train_df, test_df).\n    10. **AutoBalance_Agent**: Balance the TRAINING set only (train_df -> train_df).\n        *DO NOT touch test_df in this step.*\n\n    **OUTPUT RULES:**\n    1. Do not output the actual dataframes as text (they are too large).\n    2. Once Step 10 is finished, output a final confirmation: \n       \"‚úÖ Pipeline Complete. Variables 'train_df' and 'test_df' are ready for modeling.\"\n\n    Failure to follow the sequence will result in immediate termination.\n    \"\"\",\n    tools=[\n        AgentTool(agent=standardize_agent),\n        AgentTool(agent=Date_Agent),\n        AgentTool(agent=duplicates_agent),\n        AgentTool(agent=grouper_agent),\n        AgentTool(agent=Null_agent),\n        AgentTool(agent=Correlation_Agent),\n        AgentTool(agent=Skew_Agent),\n        AgentTool(agent=Encoding_Agent),\n        AgentTool(agent=Scaler_Agent),\n        AgentTool(agent=AutoBalance_Agent),\n    ],\n)\n\nprint(\"‚úÖ The Bane of Interns is online.ü§ñ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:47.221137Z","iopub.execute_input":"2025-12-01T04:17:47.222156Z","iopub.status.idle":"2025-12-01T04:17:47.243461Z","shell.execute_reply.started":"2025-12-01T04:17:47.222118Z","shell.execute_reply":"2025-12-01T04:17:47.242328Z"}},"outputs":[{"name":"stdout","text":"‚úÖ The Bane of Interns is online.ü§ñ\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Save a copy for comparing later\ndf_raw_snapshot = df.copy()\n# Execute\nrunner = InMemoryRunner(agent=Mastermind)\nresponse = await runner.run_debug(\"Start the pipeline.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:19:46.384124Z","iopub.execute_input":"2025-12-01T04:19:46.384473Z","iopub.status.idle":"2025-12-01T04:21:33.760497Z","shell.execute_reply.started":"2025-12-01T04:19:46.384449Z","shell.execute_reply":"2025-12-01T04:21:33.759557Z"}},"outputs":[{"name":"stdout","text":"\n ### Created new session: debug_session_id\n\nUser > Start the pipeline.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > Okay, let's start the data engineering pipeline.\n\n**Step 1: standardize_agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 2: Date_Agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 3: duplicates_agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 4: grouper_agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 5: Null_agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 6: Correlation_Agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 7: Skew_Agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 8: Encoding_Agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 9: Scaler_Agent**\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Mastermind_agent > **Step 10: AutoBalance_Agent**\n\nMastermind_agent > ‚úÖ Pipeline Complete. Variables 'train_df' and 'test_df' are ready for modeling.\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Time to compare\n\ndef print_comparison_report(raw, train, test):\n    print(\"\\n\" + \"=\"*40)\n    print(\"üß™ PIPELINE VALIDATION REPORT\")\n    print(\"=\"*40)\n\n    # 1. SHAPE & DROPS\n    total_final = len(train) + len(test)\n    dropped = len(raw) - total_final\n    print(f\"\\n1. DATA VOLUME:\")\n    print(f\"   - Original: {len(raw)} rows\")\n    print(f\"   - Final:    {total_final} rows\")\n    print(f\"   - Dropped:  {dropped} rows ({(dropped/len(raw)):.1%} loss)\")\n\n    # 2. NULLS\n    print(f\"\\n2. NULL CHECK:\")\n    print(f\"   - Remaining Nulls: {train.isnull().sum().sum()} (Must be 0)\")\n\n    # 3. COLUMNS (Encoding check)\n    print(f\"\\n3. DIMENSIONS:\")\n    print(f\"   - Orig Cols:  {raw.shape[1]}\")\n    print(f\"   - Final Cols: {train.shape[1]}\")\n    \n    # 4. SKEW (Log Transform Check)\n    if 'Transaction_Amt' in raw.columns and 'Transaction_Amt' in train.columns:\n        print(f\"\\n4. SKEW CORRECTION:\")\n        print(f\"   - Orig Max:  ${raw['Transaction_Amt'].max():,.2f}\")\n        print(f\"   - Final Max: {train['Transaction_Amt'].max():.4f} (Scaled)\")\n\n    # 5. BALANCE CHECK\n    target_col = 'Target_Label'\n    if target_col in train.columns:\n        print(f\"\\n5. CLASS BALANCE ({target_col}):\")\n        tc = train[target_col].value_counts(normalize=True)\n        print(f\"   - Train (Balanced): 0: {tc.get(0,0):.2f} | 1: {tc.get(1,0):.2f}\")\n        \n        testc = test[target_col].value_counts(normalize=True)\n        print(f\"   - Test (Natural):   0: {testc.get(0,0):.2f} | 1: {testc.get(1,0):.2f}\")\n\ntrain_abs_path = '/kaggle/working/train_df.pkl'\ntest_abs_path = '/kaggle/working/test_df.pkl'\n\nif os.path.exists(train_abs_path):\n    print(f\"\\n‚úÖ SUCCESS: Found {train_abs_path}\")\n    final_train = pd.read_pickle(train_abs_path)\n    final_test = pd.read_pickle(test_abs_path)\n    print_comparison_report(df_raw_snapshot, final_train, final_test)\nelse:\n    print(f\"\\n‚ùå FAILURE: File still missing at {train_abs_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:23:39.476578Z","iopub.execute_input":"2025-12-01T04:23:39.476904Z","iopub.status.idle":"2025-12-01T04:23:39.493343Z","shell.execute_reply.started":"2025-12-01T04:23:39.476881Z","shell.execute_reply":"2025-12-01T04:23:39.492308Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ SUCCESS: Found /kaggle/working/train_df.pkl\n\n========================================\nüß™ PIPELINE VALIDATION REPORT\n========================================\n\n1. DATA VOLUME:\n   - Original: 1005 rows\n   - Final:    1288 rows\n   - Dropped:  -283 rows (-28.2% loss)\n\n2. NULL CHECK:\n   - Remaining Nulls: 0 (Must be 0)\n\n3. DIMENSIONS:\n   - Orig Cols:  14\n   - Final Cols: 24\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def deep_dive_report(raw, train):\n    print(\"\\n\" + \"=\"*60)\n    print(\"üî¨ DEEP DIVE: TRANSFORMATION AUDIT\")\n    print(\"=\"*60)\n\n    # 1. STANDARDIZATION (Text Cleaning)\n    # Compare raw 'City' to processed 'city'\n    print(\"\\n1. üßπ STANDARDIZATION (Text Cleaning)\")\n    print(\"-\" * 30)\n    try:\n        # Get top 5 unique values to show consolidation\n        raw_cities = raw['City'].unique()[:5]\n        clean_cities = train['city'].unique()[:5] if 'city' in train.columns else \"Column Dropped/Encoded\"\n        print(f\"üî¥ Raw Cities:   {raw_cities}\")\n        print(f\"üü¢ Clean Cities: {clean_cities}\")\n    except Exception as e:\n        print(f\"Could not compare cities: {e}\")\n\n    # 2. DATE ENGINEERING\n    # Show how 'Join_Date' exploded into features\n    print(\"\\n2. üìÖ DATE ENGINEERING\")\n    print(\"-\" * 30)\n    if 'Join_Date' in raw.columns:\n        print(f\"üî¥ Raw Date (Row 0): '{raw['Join_Date'].iloc[0]}'\")\n        \n        # Find new columns starting with 'join_date'\n        date_cols = [c for c in train.columns if 'join_date' in c]\n        if date_cols:\n            print(f\"üü¢ Extracted Features (Row 0):\")\n            print(train[date_cols].iloc[0].to_frame().T.to_string(index=False))\n        else:\n            print(\"‚ö†Ô∏è No date features found.\")\n\n    # 3. FEATURE SELECTION (Correlation)\n    # Check if Temp_F (redundant) is gone but Temp_C remains\n    print(\"\\n3. ‚úÇÔ∏è FEATURE SELECTION (Correlation)\")\n    print(\"-\" * 30)\n    has_temp_c = 'temp_c' in train.columns\n    has_temp_f = 'temp_f' in train.columns\n    \n    if has_temp_c and not has_temp_f:\n        print(\"‚úÖ SUCCESS: 'temp_c' kept, 'temp_f' DROPPED (Correlation > 0.95 detected).\")\n    elif has_temp_f:\n        print(\"‚ö†Ô∏è CHECK: 'temp_f' still exists.\")\n    else:\n        print(\"‚ÑπÔ∏è Note: Temp columns not found (maybe renamed?).\")\n\n    # 4. SCALING (Z-Score)\n    # Compare distributions of Transaction Amount\n    print(\"\\n4. ‚öñÔ∏è SCALING & SKEW (Transaction_Amt)\")\n    print(\"-\" * 30)\n    if 'Transaction_Amt' in raw.columns and 'transaction_amt' in train.columns:\n        raw_stats = raw['Transaction_Amt'].describe()\n        new_stats = train['transaction_amt'].describe()\n        \n        print(f\"{'STAT':<10} | {'RAW (Skewed)':<15} | {'FINAL (Log+Scaled)':<15}\")\n        print(\"-\" * 45)\n        print(f\"{'Mean':<10} | {raw_stats['mean']:<15.2f} | {new_stats['mean']:<15.4f} (Should be ~0)\")\n        print(f\"{'Std':<10} | {raw_stats['std']:<15.2f} | {new_stats['std']:<15.4f} (Should be ~1)\")\n        print(f\"{'Max':<10} | {raw_stats['max']:<15.2f} | {new_stats['max']:<15.4f}\")\n\n    # 5. ENCODING\n    # Show how Membership (Gold/Silver) became numbers or One-Hot\n    print(\"\\n5. üî† ENCODING\")\n    print(\"-\" * 30)\n    # Check for One-Hot columns\n    membership_cols = [c for c in train.columns if 'membership' in c]\n    if len(membership_cols) > 1:\n        print(f\"‚úÖ One-Hot Detected. 'Membership' expanded to: {membership_cols}\")\n        print(train[membership_cols].head(3))\n    elif len(membership_cols) == 1:\n         print(f\"‚úÖ Label Encoding Detected. 'Membership' is now numeric.\")\n         print(train[membership_cols].head(3))\n\n    # 6. BALANCING\n    print(\"\\n6. ‚öñÔ∏è CLASS IMBALANCE (Oversampling)\")\n    print(\"-\" * 30)\n    orig_counts = raw['Target_Label'].value_counts()\n    new_counts = train['target_label'].value_counts()\n    \n    print(f\"üî¥ Original Ratio: 1s are {orig_counts[1] / len(raw):.1%} of data\")\n    print(f\"üü¢ Training Ratio: 1s are {new_counts[1] / len(train):.1%} of data (Target ~50%)\")\n    print(f\"   (Synthetic samples added: {len(train) - len(raw)})\")\n\n# RUN IT\nif os.path.exists('/kaggle/working/train_df.pkl'):\n    final_train = pd.read_pickle('/kaggle/working/train_df.pkl')\n    # Use df_raw_snapshot from the very beginning of your script\n    deep_dive_report(df_raw_snapshot, final_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:55:06.474114Z","iopub.execute_input":"2025-12-01T04:55:06.474483Z","iopub.status.idle":"2025-12-01T04:55:06.480979Z","shell.execute_reply.started":"2025-12-01T04:55:06.474460Z","shell.execute_reply":"2025-12-01T04:55:06.480093Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# I am just a recent CS grad, so the solutions provided here are a far cry from perfect,\n# but if there are any questions, please contact me at https://www.linkedin.com/in/yulin-lin-0a05201ab/.","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}