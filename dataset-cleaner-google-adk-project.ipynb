{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af43453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:45:52.921454Z",
     "iopub.status.busy": "2025-12-01T07:45:52.921123Z",
     "iopub.status.idle": "2025-12-01T07:45:52.926087Z",
     "shell.execute_reply": "2025-12-01T07:45:52.925336Z"
    },
    "papermill": {
     "duration": 0.014073,
     "end_time": "2025-12-01T07:45:52.927499",
     "exception": false,
     "start_time": "2025-12-01T07:45:52.913426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: honestly, I was pretty tempted to create a model that preps data based on type of data and model it will feed into,\n",
    "# but opted to choose this simplified generalized version due to time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5dadb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:45:52.938603Z",
     "iopub.status.busy": "2025-12-01T07:45:52.938323Z",
     "iopub.status.idle": "2025-12-01T07:45:52.942385Z",
     "shell.execute_reply": "2025-12-01T07:45:52.941622Z"
    },
    "papermill": {
     "duration": 0.011188,
     "end_time": "2025-12-01T07:45:52.943715",
     "exception": false,
     "start_time": "2025-12-01T07:45:52.932527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the summary of agents used and in order:\n",
    "# Standardize: Fix types, remove currency symbols, unify text casing.\n",
    "# Date: Extract features (Year/Month) from timestamps.\n",
    "# Duplicates: Remove exact row matches (Safety: <1%).\n",
    "# Grouper: Group rare categories into \"Other\" (Safety: <1%).\n",
    "# Nulls: Drop bad cols/rows or Impute (Median/Mode).\n",
    "# Correlation: Drop redundant features (Correlation > 95%).\n",
    "# Skew: Log-transform positive outliers.\n",
    "# Encoding: Convert categories to numbers (One-Hot vs Label).\n",
    "# Scaler: Split Train/Test and Standardize (Prevent Leakage).\n",
    "# Bias: Fixes Statistical Bias and Class Imbalance on Train set.\n",
    "# Mastermind: orchestrates the code of these 10 agents to work in sequential order on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cffc48a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:45:52.954474Z",
     "iopub.status.busy": "2025-12-01T07:45:52.954160Z",
     "iopub.status.idle": "2025-12-01T07:45:53.196770Z",
     "shell.execute_reply": "2025-12-01T07:45:53.195822Z"
    },
    "papermill": {
     "duration": 0.249825,
     "end_time": "2025-12-01T07:45:53.198279",
     "exception": false,
     "start_time": "2025-12-01T07:45:52.948454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup and authentication complete.\n"
     ]
    }
   ],
   "source": [
    "# setup keys\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    print(\"‚úÖ Setup and authentication complete.\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"üîë Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f6bb17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:45:53.209867Z",
     "iopub.status.busy": "2025-12-01T07:45:53.209031Z",
     "iopub.status.idle": "2025-12-01T07:46:34.362656Z",
     "shell.execute_reply": "2025-12-01T07:46:34.361701Z"
    },
    "papermill": {
     "duration": 41.16507,
     "end_time": "2025-12-01T07:46:34.368379",
     "exception": false,
     "start_time": "2025-12-01T07:45:53.203309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ADK components imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from google.genai import types\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools import google_search, AgentTool, ToolContext\n",
    "from google.adk.code_executors import BuiltInCodeExecutor\n",
    "from google.adk.code_executors import UnsafeLocalCodeExecutor\n",
    "\n",
    "print(\"‚úÖ ADK components imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb657f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.380519Z",
     "iopub.status.busy": "2025-12-01T07:46:34.379305Z",
     "iopub.status.idle": "2025-12-01T07:46:34.386051Z",
     "shell.execute_reply": "2025-12-01T07:46:34.384945Z"
    },
    "papermill": {
     "duration": 0.014131,
     "end_time": "2025-12-01T07:46:34.387495",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.373364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure retry options\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568e385a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.398874Z",
     "iopub.status.busy": "2025-12-01T07:46:34.398571Z",
     "iopub.status.idle": "2025-12-01T07:46:34.451231Z",
     "shell.execute_reply": "2025-12-01T07:46:34.449965Z"
    },
    "papermill": {
     "duration": 0.060287,
     "end_time": "2025-12-01T07:46:34.452686",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.392399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Generating The Doomed Dataset...\n",
      "‚úÖ Dataset Created. Shape: (1005, 14)\n",
      "‚úÖ SETUP: Messy dataframe created at '/kaggle/working/df.pkl'\n",
      "Original Columns: ['ID_Column', 'Transaction_Amt', 'Temp_C', 'Temp_F', 'City', 'Salary', 'Join_Date', 'Garbage_Col', 'Sensor_Reading', 'User_Age', 'Referral', 'Zip_Code', 'Membership', 'Target_Label']\n",
      "------------------------------\n",
      "Columns: ['ID_Column', 'Transaction_Amt', 'Temp_C', 'Temp_F', 'City', 'Salary', 'Join_Date', 'Garbage_Col', 'Sensor_Reading', 'User_Age', 'Referral', 'Zip_Code', 'Membership', 'Target_Label']\n",
      "   ID_Column  Transaction_Amt     Temp_C     Temp_F      City  Salary  \\\n",
      "0          0         4.870722  25.988750  78.779749        SF   91833   \n",
      "1          1         6.984778  26.684181  80.031525  New York  $57890   \n",
      "2          2         0.872671  22.011714  71.621085   chicago  111204   \n",
      "3          3        38.102154  26.689683  80.041429    Austin   82824   \n",
      "4          4         1.229447  21.349859  70.429747       sf   124235   \n",
      "\n",
      "    Join_Date  Garbage_Col  Sensor_Reading  User_Age Referral Zip_Code  \\\n",
      "0  Not a Date          1.0             NaN       NaN   Direct    90012   \n",
      "1  2020-01-02          1.0             NaN       NaN   Google    90002   \n",
      "2  2020-01-03          1.0             NaN       NaN   Google    90004   \n",
      "3  2020-01-04          1.0             NaN       NaN   Direct    90025   \n",
      "4  2020-01-05          1.0             NaN       NaN   Google    90043   \n",
      "\n",
      "  Membership  Target_Label  \n",
      "0     Silver             0  \n",
      "1     Bronze             0  \n",
      "2     Bronze             0  \n",
      "3     Bronze             0  \n",
      "4     Bronze             0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# using a randomized dataset, in production replace with an actual dataset\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(2)\n",
    "\n",
    "def generate_messy_dataset(rows=1000):\n",
    "    print(\"‚ö†Ô∏è Generating The Doomed Dataset...\")\n",
    "    \n",
    "    # 1. BASE DATA & SKEW (Triggers SkewAgent)\n",
    "    # Generate a log-normal distribution (Right skewed)\n",
    "    transaction_amt = np.random.lognormal(mean=2, sigma=1, size=rows)\n",
    "    \n",
    "    # 2. REDUNDANT FEATURES (Triggers CorrelationAgent)\n",
    "    # Celsius and Fahrenheit are perfectly correlated\n",
    "    temp_c = np.random.normal(25, 5, rows)\n",
    "    temp_f = temp_c * 9/5 + 32\n",
    "    \n",
    "    # 3. MESSY STRINGS & CURRENCY (Triggers StandardizeAgent)\n",
    "    # Includes whitespace, different cases, and symbols\n",
    "    cities = [\"  new york \", \"New York\", \"SF\", \"sf \", \"chicago\", \"Chicago\", \"  Austin\"]\n",
    "    city_col = np.random.choice(cities, rows)\n",
    "    \n",
    "    salaries = np.random.randint(40000, 150000, rows).astype(str)\n",
    "    # Corrupt 30% of salaries with currency symbols\n",
    "    for i in range(rows):\n",
    "        if np.random.rand() < 0.3:\n",
    "            salaries[i] = f\"${salaries[i]}\"\n",
    "        if np.random.rand() < 0.1:\n",
    "            salaries[i] = f\"{salaries[i]},00\" # European style comma/decimal mix\n",
    "            \n",
    "    # 4. DATES (Triggers DateAgent)\n",
    "    # Mix of formats and NaTs\n",
    "    start_date = pd.to_datetime('2020-01-01')\n",
    "    dates = [start_date + pd.Timedelta(days=x) for x in range(rows)]\n",
    "    date_strings = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "    # Corrupt some dates\n",
    "    date_strings[0] = \"Not a Date\"\n",
    "    date_strings[10] = \"Unknown\"\n",
    "    \n",
    "    # 5. NULLS & MISSING DATA (Triggers NullAgent)\n",
    "    # A. > 50% Missing (Should be dropped entirely)\n",
    "    mostly_empty = np.array([np.nan] * rows)\n",
    "    mostly_empty[:10] = 1 # Only 10 values exist\n",
    "    \n",
    "    # B. < 5% Missing (Rows should be dropped)\n",
    "    tiny_missing = np.random.rand(rows)\n",
    "    tiny_missing[:15] = np.nan # 1.5% missing\n",
    "    \n",
    "    # C. ~20% Missing (Should be Imputed)\n",
    "    medium_missing_age = np.random.randint(18, 70, rows).astype(float)\n",
    "    medium_missing_age[:200] = np.nan # 20% missing\n",
    "    \n",
    "    # 6. RARE CATEGORIES (Triggers GrouperAgent)\n",
    "    # 'Google' and 'Direct' are common; 'Friend' and 'Billboard' are rare (<1%)\n",
    "    sources = ['Google']*800 + ['Direct']*190 + ['Friend']*5 + ['Billboard']*5\n",
    "    np.random.shuffle(sources)\n",
    "    \n",
    "    # 7. HIGH CARDINALITY (Triggers EncodingAgent - Label Encode)\n",
    "    # 50 unique ZIP codes\n",
    "    zips = np.random.randint(90000, 90050, rows).astype(str)\n",
    "    \n",
    "    # 8. LOW CARDINALITY (Triggers EncodingAgent - One-Hot Encode)\n",
    "    membership = np.random.choice(['Gold', 'Silver', 'Bronze'], rows)\n",
    "    \n",
    "    # 9. CLASS IMBALANCE (Triggers AutoBalanceAgent)\n",
    "    # 90% Class 0, 10% Class 1\n",
    "    target = np.random.choice([0, 1], rows, p=[0.90, 0.10])\n",
    "    \n",
    "    # CREATE DATAFRAME\n",
    "    df = pd.DataFrame({\n",
    "        'ID_Column': range(rows), # Should be ignored by Skew/Scaling agents\n",
    "        'Transaction_Amt': transaction_amt, # Skewed\n",
    "        'Temp_C': temp_c, # Redundant\n",
    "        'Temp_F': temp_f, # Redundant to be dropped\n",
    "        'City': city_col, # Messy text\n",
    "        'Salary': salaries, # Messy numbers ($)\n",
    "        'Join_Date': date_strings, # Date parsing\n",
    "        'Garbage_Col': mostly_empty, # >50% null\n",
    "        'Sensor_Reading': tiny_missing, # <5% null\n",
    "        'User_Age': medium_missing_age, # Impute median\n",
    "        'Referral': sources, # Group 'Friend' -> Other\n",
    "        'Zip_Code': zips, # Label Encode\n",
    "        'Membership': membership, # One-Hot Encode\n",
    "        'Target_Label': target # Imbalanced\n",
    "    })\n",
    "    \n",
    "    # 10. DUPLICATES (Triggers DuplicatesAgent)\n",
    "    # Append top 5 rows to bottom to create exact duplicates (0.5% duplicates)\n",
    "    df = pd.concat([df, df.head(5)], ignore_index=True)\n",
    "    \n",
    "    print(\"‚úÖ Dataset Created. Shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "# Initialize\n",
    "df = generate_messy_dataset()\n",
    "\n",
    "# Save to the bridge location\n",
    "df.to_pickle('/kaggle/working/df.pkl')\n",
    "print(\"‚úÖ SETUP: Messy dataframe created at '/kaggle/working/df.pkl'\")\n",
    "print(f\"Original Columns: {df.columns.tolist()}\")\n",
    "print(\"-\" * 30)\n",
    "df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "local_executor = UnsafeLocalCodeExecutor(working_dir='/kaggle/working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dede2d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.463868Z",
     "iopub.status.busy": "2025-12-01T07:46:34.463575Z",
     "iopub.status.idle": "2025-12-01T07:46:34.469893Z",
     "shell.execute_reply": "2025-12-01T07:46:34.468944Z"
    },
    "papermill": {
     "duration": 0.013677,
     "end_time": "2025-12-01T07:46:34.471302",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.457625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standardize text based on appearance percentage and similarity, strings to number if possible\n",
    "standardize_agent = LlmAgent(\n",
    "    name=\"StandardizeAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert.\n",
    "    \n",
    "    **YOUR GOAL:**\n",
    "    Write a generic, robust Python script to clean and standardize ANY dataset without knowing column names in advance.\n",
    "\n",
    "    **CRITICAL SETUP:**\n",
    "    1. Imports: `import pandas as pd; import numpy as np; import warnings; warnings.filterwarnings('ignore')`\n",
    "    2. Load: `df = pd.read_pickle('/kaggle/working/df.pkl')`\n",
    "\n",
    "    **ADAPTIVE TRANSFORMATION LOGIC:**\n",
    "    \n",
    "    1. **Step 1: Clean Headers**\n",
    "       - Strip whitespace, lowercase, and replace spaces with underscores for all column names.\n",
    "\n",
    "    2. **Step 2: Smart Type Inference (Iterate through OBJECT columns only)**\n",
    "       For each column where `dtype == 'object'`, perform the following checks IN ORDER:\n",
    "\n",
    "       * **A. Check for Numeric/Currency:**\n",
    "           - Create a temporary clean version: Remove '$', ',', and whitespace.\n",
    "           - **DO NOT REMOVE '.' (Decimal Points) or '-' (Negative Signs).**\n",
    "             - *Hint:* Use regex `r'[$,]'` to remove only specific symbols, NOT `r'[^\\d]'`.\n",
    "           - Attempt convert: `temp = pd.to_numeric(clean_col, errors='coerce')`\n",
    "           - **The Safety Check:** Calculate the ratio of Non-Null values in `temp` vs the original column.\n",
    "           - **Decision:** IF `temp.notna().mean()` > 0.8 (meaning >80% of data successfully converted):\n",
    "             - Apply the conversion to the actual column.\n",
    "             - Continue to next column (do not check Date).\n",
    "\n",
    "       * **B. Check for Date/Time:**\n",
    "           - Attempt convert: `temp = pd.to_datetime(col, errors='coerce')`\n",
    "           - **The Safety Check:** Calculate the ratio of Non-Null values.\n",
    "           - **Decision:** IF `temp.notna().mean()` >= 0.6 (meaning >60% is a valid date):\n",
    "             - Apply the conversion.\n",
    "             - Continue to next column.\n",
    "\n",
    "       * **C. Fallback: Text Cleaning:**\n",
    "           - If neither Numeric nor Date checks pass:\n",
    "             - Strip whitespace: `df[col] = df[col].astype(str).str.strip()`\n",
    "             - Title case: `df[col] = df[col].str.title()`\n",
    "             - Replace empty strings with `np.nan`.\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    - Output the Python code block to perform the transformation.\n",
    "    - End with: `print(df.dtypes); df.to_pickle('/kaggle/working/df.pkl'); print(\"Standardized.\")`\n",
    "    - After the code executes successfully (and prints the result), you MUST respond with the text: \"Standardization complete.\"\n",
    "    \"\"\",\n",
    "    code_executor=local_executor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b1fdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.483250Z",
     "iopub.status.busy": "2025-12-01T07:46:34.482392Z",
     "iopub.status.idle": "2025-12-01T07:46:34.487984Z",
     "shell.execute_reply": "2025-12-01T07:46:34.487298Z"
    },
    "papermill": {
     "duration": 0.012986,
     "end_time": "2025-12-01T07:46:34.489301",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.476315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  feature creation ex - spilt dates\n",
    "Date_Agent = LlmAgent(\n",
    "    name=\"DateAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `pandas` to Create Features by splitting Date/Time columns in the variable `df`.\n",
    "\n",
    "    **CRITICAL:**\n",
    "        1. Load 'df.pkl'. Do NOT create dummy data. Extract features from 'df.pkl', overwrite 'df.pkl'.\n",
    "        2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    - load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "\n",
    "    **LOGIC & SAFETY CHECKS:**\n",
    "    1. **Identify Candidates:**\n",
    "       - Iterate through all columns.\n",
    "       - Target columns where:\n",
    "         - Dtype is already `datetime`.\n",
    "         - OR Name contains: \"date\", \"time\", \"joined\", \"created\", \"at\" (case-insensitive) AND Dtype is `object`.\n",
    "    \n",
    "    2. **Safe Conversion:**\n",
    "       - For candidates, attempt: `temp = pd.to_datetime(df[col], errors='coerce')`\n",
    "       - **Validation:** Check the NaT (Null) rate of `temp`.\n",
    "         - IF NaT rate > 50%: The column is likely NOT a real date. **SKIP IT.**\n",
    "         - IF NaT rate <= 50%: Assign `df[col] = temp` and proceed to step 3.\n",
    "\n",
    "    3. **Feature Splitting (The \"Creation\" Step):**\n",
    "       - For every valid date column:\n",
    "         - Create `{col}_year`: `df[col].dt.year`\n",
    "         - Create `{col}_month`: `df[col].dt.month`\n",
    "         - Create `{col}_day`: `df[col].dt.day`\n",
    "         - Create `{col}_dow`: `df[col].dt.dayofweek` (0=Mon, 6=Sun)\n",
    "         - Create `{col}_is_weekend`: `(df[col].dt.dayofweek >= 5).astype(int)`\n",
    "\n",
    "    4. **Cleanup:**\n",
    "       - DROP the original date column after extracting features (Models cannot digest raw timestamps).\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. End with:\n",
    "        1. `print(f\"Date Features Created. New Shape: {df.shape}\")` and `print(df.columns.tolist())`.\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Date features added. Saved to df.pkl\")\n",
    "    3. Use ONLY standard libraries and `pandas`.\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Date extraction complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2e4ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.500859Z",
     "iopub.status.busy": "2025-12-01T07:46:34.500230Z",
     "iopub.status.idle": "2025-12-01T07:46:34.505398Z",
     "shell.execute_reply": "2025-12-01T07:46:34.504689Z"
    },
    "papermill": {
     "duration": 0.012357,
     "end_time": "2025-12-01T07:46:34.506650",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.494293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove duplicates data <1% for safety\n",
    "duplicates_agent = LlmAgent(\n",
    "    name=\"DuplicatesAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `pandas` to remove duplicate data safely from a variable `df`.\n",
    "\n",
    "    **CRITICAL:** Load 'df.pkl'. Remove duplicates from 'df.pkl', overwrite 'df.pkl'.\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "    2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **LOGIC & SAFETY CHECKS:**\n",
    "    1. **Calculate Duplicates:** Identify how many rows would be removed using exact row matching.\n",
    "    2. **The 1% Safety Rule:** - Calculate the drop percentage: `(duplicates_count / total_rows) * 100`.\n",
    "       - IF the drop percentage is **greater than 1%**: Do NOT modify `df`. Instead, print(f\"Aborting: Duplicates exceed 1% safety limit. Found {pct}%\")`.\n",
    "       - IF the drop percentage is **less than or equal to 1%**: Remove the duplicates permanently from `df`.\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. End with:\n",
    "        1. printing: `print(f\"Successfully dropped {dropped_count} rows. New shape: {df.shape}\")`\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Duplicates removed. Saved to df.pkl\")\n",
    "    3. Use ONLY standard libraries and `pandas`.\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Duplicate removal complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaadee66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.517790Z",
     "iopub.status.busy": "2025-12-01T07:46:34.517188Z",
     "iopub.status.idle": "2025-12-01T07:46:34.522289Z",
     "shell.execute_reply": "2025-12-01T07:46:34.521536Z"
    },
    "papermill": {
     "duration": 0.012062,
     "end_time": "2025-12-01T07:46:34.523549",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.511487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# groups rare categories into \"other\" <1%, 0.5% individually\n",
    "grouper_agent = LlmAgent(\n",
    "    name=\"GrouperAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    **GOAL:** Group rare categories (appearing < 0.5%) into 'Other' if the total impact is < 1%.\n",
    "\n",
    "    **CRITICAL:** Load 'df.pkl'. Group rare categories from 'df.pkl', overwrite 'df.pkl'.\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "    2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **LOGIC:**\n",
    "    1. Iterate through columns where `dtype == 'object'`.\n",
    "    2. For each column:\n",
    "       - Calculate value counts (normalized).\n",
    "       - Identify \"rare_values\": those with `frequency < 0.005` (0.5%).\n",
    "       - Calculate \"impact\": Sum of frequencies of all `rare_values`.\n",
    "    3. **Decision Rule:**\n",
    "       - **IF impact <= 0.01 (1%):** - Replace `rare_values` with the string \"Other\".\n",
    "         - Print: `f\"Column '{col}': Grouped {count} categories ({impact:.2%})\"`\n",
    "       - **ELSE (impact > 1%):**\n",
    "         - Print: `f\"Column '{col}': Skipped (Impact {impact:.2%} > 1% safety limit)\"`\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. The code MUST end with:\n",
    "        1. A summary print for each column: `print(f\"Column '{col}': Grouped {count} rows\")`.\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Grouping complete. Saved to df.pkl\")\n",
    "    3. Use ONLY standard libraries and `pandas`.\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Duplicate removal complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a80d91b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.534852Z",
     "iopub.status.busy": "2025-12-01T07:46:34.534253Z",
     "iopub.status.idle": "2025-12-01T07:46:34.539424Z",
     "shell.execute_reply": "2025-12-01T07:46:34.538729Z"
    },
    "papermill": {
     "duration": 0.012199,
     "end_time": "2025-12-01T07:46:34.540593",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.528394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows if the missing data is minimal (<5%), columns if the feature is mostly empty (>50%)\n",
    "# Use median to fill in if possible\n",
    "# total thresholding to prevent cascading data loss\n",
    "Null_agent = LlmAgent(\n",
    "    name=\"NullAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `pandas` to handle missing data (nulls).\n",
    "\n",
    "    **CRITICAL:** Load 'df.pkl'. Handle nulls in 'df.pkl', overwrite 'df.pkl'.\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "    2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **LOGIC - EXECUTE IN THIS ORDER:**\n",
    "    1. **Column Cleanup:**\n",
    "       - IF a column is missing > 50% data: Drop the **Column**.\n",
    "\n",
    "    2. **Total Row Safety Check:**\n",
    "       - Identify ALL rows that contain nulls in the remaining columns.\n",
    "       - Calculate `total_rows_with_nulls`.\n",
    "       - Calculate `loss_pct = (total_rows_with_nulls / total_rows) * 100`.\n",
    "\n",
    "    3. **Decision Branch:**\n",
    "       - **IF loss_pct =< 5%:**\n",
    "         - Drop ALL rows containing nulls. (Safe to drop).\n",
    "       - **ELSE (If loss_pct > 5%):**\n",
    "         - Do NOT drop rows. Instead, Impute (Fill) data:\n",
    "         - Numerics -> Fill with Median.\n",
    "         - Object/String -> Fill with Mode (Top value).\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. End with: \n",
    "        1. `print(f\"Action taken: {'Dropped Rows' if loss < 5 else 'Imputed Data'}. New Shape: {df.shape}\")`\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Redundancy removed. Saved to df.pkl\")\n",
    "    3. Use ONLY standard libraries and `pandas`.\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Nulls handled.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12adfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.551893Z",
     "iopub.status.busy": "2025-12-01T07:46:34.551273Z",
     "iopub.status.idle": "2025-12-01T07:46:34.556434Z",
     "shell.execute_reply": "2025-12-01T07:46:34.555699Z"
    },
    "papermill": {
     "duration": 0.012287,
     "end_time": "2025-12-01T07:46:34.557711",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.545424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA - correlation matrix, drops var/features with little relevance\n",
    "Correlation_Agent = LlmAgent(\n",
    "    name=\"CorrelationAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `pandas` and `numpy` to remove **Redundant Features** (Multicollinearity) based on a correlation matrix.\n",
    "\n",
    "    **CRITICAL:** Load 'df.pkl'. Drop correlated cols in 'df.pkl', overwrite 'df.pkl'.\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "    2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **LOGIC:**\n",
    "    1. Select numeric columns: `nums = df.select_dtypes(include=[np.number])`\n",
    "    2. Safety Check: If `nums.shape[1] < 2`, stop and save.\n",
    "    3. **Calculate Matrix:** `corr_matrix = nums.corr().abs()`\n",
    "    4. **Select Upper Triangle (CRITICAL):** - Use this logic to avoid self-correlation:\n",
    "       - `upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))`\n",
    "    5. **Identify Drops:**\n",
    "       - Find columns where any value in `upper` is > 0.95.\n",
    "       - `to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]`\n",
    "    6. **Drop:** `df = df.drop(columns=to_drop)`\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. End with:\n",
    "        1.`print(f\"Dropped {len(to_drop)} redundant features: {to_drop}. New Shape: {df.shape}\")`\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Redundancy removed. Saved to df.pkl\")\n",
    "    3. Use ONLY standard libraries, `numpy`, and `pandas`.\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"PCA complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11acf261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.569121Z",
     "iopub.status.busy": "2025-12-01T07:46:34.568380Z",
     "iopub.status.idle": "2025-12-01T07:46:34.574133Z",
     "shell.execute_reply": "2025-12-01T07:46:34.573276Z"
    },
    "papermill": {
     "duration": 0.013083,
     "end_time": "2025-12-01T07:46:34.575621",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.562538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for numeric values, if skew() > 1, log transform for outliers (log(x+1)) if no neg values\n",
    "# exclude ID, dates, and target\n",
    "Skew_Agent = LlmAgent(\n",
    "    name=\"SkewAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `pandas` and `numpy` to fix positive skewness in the variable `df`.\n",
    "\n",
    "    **CRITICAL:** Load 'df.pkl'. Fix skew in 'df.pkl', overwrite 'df.pkl'.\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "    2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **LOGIC:**\n",
    "    1. Iterate through **Numeric Columns** (float/int).\n",
    "    2. **Exclusion Check:** - SKIP if name contains 'id', 'target', or 'label' (case insensitive).\n",
    "       - SKIP if name ends with '_year', '_month', '_day', '_dow', '_weekend'.\n",
    "    3. **Value Check:**\n",
    "       - SKIP if column contains negative values (Log is undefined).\n",
    "    4. **Transformation:**\n",
    "       - Calculate `old_skew = df[col].skew()`.\n",
    "       - **IF old_skew > 1:**\n",
    "         - Apply: `df[col] = np.log1p(df[col])`\n",
    "         - Recalculate: `new_skew = df[col].skew()`\n",
    "         - Print: `f\"Column '{col}': Skew fixed ({old_skew:.2f} -> {new_skew:.2f})\"`\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    3. The code MUST end with:\n",
    "        1. A summary loop printing: `print(f\"Column '{col}': Skew from {old_skew} -> {new_skew}\")` for changed columns only.\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Skew fixed. Saved to df.pkl\")\n",
    "    4. Use ONLY standard libraries, `numpy`, and `pandas`.\n",
    "    5. After the code executes successfully (and prints the result), you MUST respond with the text: \"Skew fix complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6268f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.588554Z",
     "iopub.status.busy": "2025-12-01T07:46:34.587744Z",
     "iopub.status.idle": "2025-12-01T07:46:34.593404Z",
     "shell.execute_reply": "2025-12-01T07:46:34.592536Z"
    },
    "papermill": {
     "duration": 0.013518,
     "end_time": "2025-12-01T07:46:34.594821",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.581303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one hot encoding for low cardinality\n",
    "# label encoding for high and ordinal data\n",
    "Encoding_Agent = LlmAgent(\n",
    "    name=\"EncodingAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `pandas` to encode categorical data (One-Hot vs Label Encoding) in the variable `df`.\n",
    "\n",
    "    **CRITICAL:** Load 'df.pkl'. Encode 'df.pkl', overwrite 'df.pkl'.\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "    2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **LOGIC:**\n",
    "    1. **Identify Object Columns:** `obj_cols = df.select_dtypes(include=['object']).columns`\n",
    "    2. **Filter Exclusions:** Remove columns containing 'id', 'ID', or 'Id'.\n",
    "    \n",
    "    3. **Strategy Separation:**\n",
    "       - Create two lists: `one_hot_cols` and `label_cols`.\n",
    "       - Iterate through candidates:\n",
    "         - IF `df[col].nunique() < 10`: Add to `one_hot_cols`.\n",
    "         - ELSE: Add to `label_cols`.\n",
    "\n",
    "    4. **Execution:**\n",
    "       - **Step A (Label Encoding):**\n",
    "         - For col in `label_cols`: `df[col] = df[col].astype('category').cat.codes`\n",
    "       \n",
    "       - **Step B (One-Hot Encoding):**\n",
    "         - IF `one_hot_cols` is not empty:\n",
    "           - `df = pd.get_dummies(df, columns=one_hot_cols, prefix=one_hot_cols, dtype=int)`\n",
    "           - *Note:* This automatically drops original columns and appends new ones as Integers (0/1).\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. The code MUST end with:\n",
    "        1. `print(f\"Encoding Complete. New Shape: {df.shape}\")` and `print(df.dtypes)`.\n",
    "        2. df.to_pickle('/kaggle/working/df.pkl'); print(\"Encoding complete. Saved to df.pkl\")\n",
    "    3. Use ONLY standard libraries and `pandas`.\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Data Encoding complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98c974f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.606344Z",
     "iopub.status.busy": "2025-12-01T07:46:34.606022Z",
     "iopub.status.idle": "2025-12-01T07:46:34.611425Z",
     "shell.execute_reply": "2025-12-01T07:46:34.610646Z"
    },
    "papermill": {
     "duration": 0.012859,
     "end_time": "2025-12-01T07:46:34.612713",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.599854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature engineering - z score standardization for reducing scale\n",
    "# ^ spilt data train and test, prevents leakage, then transform test set using precalculated values (mean/std)\n",
    "# Not using normalization here since it's weaker to outliers and is better on Neural Nets/KNNs/Images\n",
    "Scaler_Agent = LlmAgent(\n",
    "    name=\"ScalerAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `sklearn` to split the data and perform Z-Score Standardization while preventing Data Leakage.\n",
    "\n",
    "    **CRITICAL:**\n",
    "        1. Load 'df.pkl'. DO NOT create dummy data. Save 'train_df.pkl' and 'test_df.pkl'.\n",
    "        2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "               from sklearn.model_selection import train_test_split\n",
    "               from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    - load df by: df = pd.read_pickle('/kaggle/working/df.pkl')\n",
    "\n",
    "    **LOGIC:**\n",
    "    1. **Identify Target:**\n",
    "       - Find column matching 'target' or 'label'.\n",
    "       - `target_col = [c for c in df.columns if 'target' in c.lower() or 'label' in c.lower()][0]`\n",
    "       \n",
    "    2. **Pop the Target:**\n",
    "       - `y = df[target_col]` (Save target separately)\n",
    "       - `X = df.drop(columns=[target_col])` (Remove target from features)\n",
    "       \n",
    "    3. **Split:**\n",
    "       - `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)`\n",
    "\n",
    "    4. **Scale Features (X Only - Z-Score Standardization):**\n",
    "       - Identify numeric columns in X.\n",
    "       - `numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()`\n",
    "       - **Exclude IDs:** `numeric_cols = [c for c in numeric_cols if 'id' not in c.lower()]`\n",
    "       - `scaler = StandardScaler()`\n",
    "       - `X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])`\n",
    "       - `X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])`\n",
    "\n",
    "    5. **Reassemble:**\n",
    "       - `train_df = pd.concat([X_train, y_train], axis=1)`\n",
    "       - `test_df = pd.concat([X_test, y_test], axis=1)`\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. **CRITICAL:** (Must appear exactly like this):\n",
    "        - train.to_pickle('/kaggle/working/train_df.pkl')\n",
    "        - print(\"Saved train_df.pkl\")\n",
    "        - test.to_pickle('/kaggle/working/test_df.pkl')\n",
    "        - print(\"Saved test_df.pkl\")\n",
    "        - print(\"Split & Scaled. Created train_df.pkl and test_df.pkl\")\n",
    "    3. End with:\n",
    "       - `print(f\"Split Complete. Train Shape: {train_df.shape}, Test Shape: {test_df.shape}\")`\n",
    "       - `print(\"Z-Score Standardization applied safely.\")`\n",
    "    4. After the code executes successfully (and prints the result), you MUST respond with the text: \"Data Scalar complete.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4728ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.625042Z",
     "iopub.status.busy": "2025-12-01T07:46:34.624759Z",
     "iopub.status.idle": "2025-12-01T07:46:34.630281Z",
     "shell.execute_reply": "2025-12-01T07:46:34.629468Z"
    },
    "papermill": {
     "duration": 0.01396,
     "end_time": "2025-12-01T07:46:34.631755",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.617795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fixes Statistical Bias and Class Imbalance on Train set\n",
    "# To prevent overfitting, we introduce jittering/noise injection\n",
    "# ^ but obviously exclude target so the model doesn't predict that you have 1.2 siblings or smth\n",
    "AutoBalance_Agent = LlmAgent(\n",
    "    name=\"AutoBalanceAgent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n",
    "\n",
    "    **YOUR GOAL:**\n",
    "    Write a Python script using `sklearn`, `numpy`, and `pandas` to Balance Training Data safely using **Noise Injection** (Jittering).\n",
    "\n",
    "    **CRITICAL:**\n",
    "        1. Load 'train_df.pkl'. DO NOT create dummy data. Balance 'train_df.pkl' and overwrite 'train_df.pkl'.\n",
    "        2. Start your code with:\n",
    "               import warnings\n",
    "               warnings.filterwarnings('ignore')\n",
    "\n",
    "    **INPUT CONTEXT:**\n",
    "    1. load `train_df` by: train_df = pd.read_pickle('/kaggle/working/train_df.pkl')\n",
    "\n",
    "    **LOGIC:**\n",
    "    1. **ROBUST TARGET DETECTION:**\n",
    "       - Initialize `target_col = None`\n",
    "       - **Priority 1 (Exact Match):** Check for these specific names (case-insensitive):\n",
    "         `['target_label', 'target', 'class', 'label', 'outcome', 'y']`.\n",
    "         If found, assign to `target_col` and break.\n",
    "       - **Priority 2 (Substring Match):** If `target_col` is None, check if any column name *contains* \"target\", \"label\", \"class\", or \"y\". Use the first one found.\n",
    "       - **Priority 3 (Fallback):** If still None, use the LAST column: `target_col = train_df.columns[-1]`.\n",
    "       - Print: `f\"Detected Target Column: '{target_col}'\"`\n",
    "\n",
    "    2. **Safety Checks:**\n",
    "       - **Regression Check:** If `train_df[target_col].nunique() > 20`: \n",
    "         - Print \"Regression detected (High Cardinality). Skipping balance.\"\n",
    "         - Save and Exit.\n",
    "       - **Balance Check:** Calculate `minority_count / majority_count`.\n",
    "         - If Ratio >= 0.8: Print \"Already balanced.\" -> Exit.\n",
    "\n",
    "    3. **Oversampling Strategy (The \"Synthetic Block\"):**\n",
    "       - Separate: `df_maj` (Majority) and `df_min` (Minority).\n",
    "       - Calculate needed: `n_samples = len(df_maj) - len(df_min)`.\n",
    "       - **Generate Synthetic:** `df_synthetic = resample(df_min, replace=True, n_samples=n_samples, random_state=42)`\n",
    "       \n",
    "    4. **Inject Noise (Jitter):**\n",
    "       - Iterate through **Numeric Columns** of `df_synthetic` ONLY.\n",
    "       - **Exclude:** `target_col` and any 'ID' columns.\n",
    "       - **Logic:** `df_synthetic[col] += np.random.normal(0, 0.01 * df_synthetic[col].std(), size=n_samples)`\n",
    "       - *Note:* This prevents exact duplicates.\n",
    "\n",
    "    5. **Combine:**\n",
    "       - `train_df = pd.concat([df_maj, df_min, df_synthetic])`\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Output the Python code block to perform the transformation.\n",
    "    2. End with:\n",
    "        1. train_df.to_pickle('/kaggle/working/train_df.pkl')\n",
    "        2. `print(f\"Target: '{target_col}'. Balanced with Jitter. New Shape: {train_df.shape}\")`.\n",
    "    3. After the code executes successfully (and prints the result), you MUST respond with the text: \"Bias Eliminated.\"\n",
    "\n",
    "    Failure to follow these rules will result in a system error.\n",
    "    \"\"\",\n",
    "    code_executor=local_executor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5d67d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.643887Z",
     "iopub.status.busy": "2025-12-01T07:46:34.642965Z",
     "iopub.status.idle": "2025-12-01T07:46:34.650587Z",
     "shell.execute_reply": "2025-12-01T07:46:34.649680Z"
    },
    "papermill": {
     "duration": 0.015228,
     "end_time": "2025-12-01T07:46:34.652002",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.636774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The Bane of Interns is online.ü§ñ\n"
     ]
    }
   ],
   "source": [
    "# Overall Model Orchestrator\n",
    "Mastermind = LlmAgent(\n",
    "    name=\"Mastermind_agent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n",
    "    static_instruction=\"\"\"\n",
    "    You are an efficient Data Prepping Orchestrator.\n",
    "    \n",
    "    **YOUR MISSION:**\n",
    "    Execute the following data engineering pipeline strictly in order. \n",
    "    You do not need to write code yourself; use the provided Tools to generate and execute the code.\n",
    "    All agents must read/write from the CURRENT DIRECTORY (/kaggle/working).\n",
    "    The tools will handle file I/O automatically using fixed filenames: 'df.pkl', 'train_df.pkl', 'test_df.pkl'.\n",
    "    \n",
    "    **PIPELINE SEQUENCE:**\n",
    "    1.  **standardize_agent**: Clean formatting (df -> df).\n",
    "    2.  **Date_Agent**: Extract time features (df -> df).\n",
    "    3.  **duplicates_agent**: Remove rows (df -> df).\n",
    "    4.  **grouper_agent**: Group rare categories (df -> df).\n",
    "    5.  **Null_agent**: Impute or drop missing data (df -> df).\n",
    "    6.  **Correlation_Agent**: Drop redundant features (df -> df).\n",
    "    7.  **Skew_Agent**: Fix numeric skew (df -> df).\n",
    "    8.  **Encoding_Agent**: Categorical to Numerical (df -> df).\n",
    "    9.  **Scaler_Agent**: SPLIT into Train/Test and Scale (df -> train_df, test_df).\n",
    "    10. **AutoBalance_Agent**: Balance the TRAINING set only (train_df -> train_df).\n",
    "        *DO NOT touch test_df in this step.*\n",
    "\n",
    "    **OUTPUT RULES:**\n",
    "    1. Do not output the actual dataframes as text (they are too large).\n",
    "    2. Once Step 10 is finished, output a final confirmation: \n",
    "       \"‚úÖ Pipeline Complete. Variables 'train_df' and 'test_df' are ready for modeling.\"\n",
    "\n",
    "    Failure to follow the sequence will result in immediate termination.\n",
    "    \"\"\",\n",
    "    tools=[\n",
    "        AgentTool(agent=standardize_agent),\n",
    "        AgentTool(agent=Date_Agent),\n",
    "        AgentTool(agent=duplicates_agent),\n",
    "        AgentTool(agent=grouper_agent),\n",
    "        AgentTool(agent=Null_agent),\n",
    "        AgentTool(agent=Correlation_Agent),\n",
    "        AgentTool(agent=Skew_Agent),\n",
    "        AgentTool(agent=Encoding_Agent),\n",
    "        AgentTool(agent=Scaler_Agent),\n",
    "        AgentTool(agent=AutoBalance_Agent),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ The Bane of Interns is online.ü§ñ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a081e2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:46:34.663595Z",
     "iopub.status.busy": "2025-12-01T07:46:34.663289Z",
     "iopub.status.idle": "2025-12-01T07:49:06.655909Z",
     "shell.execute_reply": "2025-12-01T07:49:06.654763Z"
    },
    "papermill": {
     "duration": 152.00046,
     "end_time": "2025-12-01T07:49:06.657506",
     "exception": false,
     "start_time": "2025-12-01T07:46:34.657046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Created new session: debug_session_id\n",
      "\n",
      "User > Start the pipeline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mastermind_agent > ‚úÖ Pipeline Complete. Variables 'train_df' and 'test_df' are ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# Save a copy for comparing later\n",
    "df_raw_snapshot = df.copy()\n",
    "# Execute\n",
    "runner = InMemoryRunner(agent=Mastermind)\n",
    "response = await runner.run_debug(\"Start the pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "842ae70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:49:06.669998Z",
     "iopub.status.busy": "2025-12-01T07:49:06.669668Z",
     "iopub.status.idle": "2025-12-01T07:49:06.683620Z",
     "shell.execute_reply": "2025-12-01T07:49:06.682374Z"
    },
    "papermill": {
     "duration": 0.022074,
     "end_time": "2025-12-01T07:49:06.685199",
     "exception": false,
     "start_time": "2025-12-01T07:49:06.663125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['id_column', 'transaction_amt', 'temp_c', 'salary', 'sensor_reading', 'user_age', 'zip_code', 'join_date_year', 'join_date_month', 'join_date_day', 'join_date_dow', 'join_date_is_weekend', 'city_Austin', 'city_Chicago', 'city_New York', 'city_Sf', 'referral_Billboard', 'referral_Direct', 'referral_Friend', 'referral_Google', 'membership_Bronze', 'membership_Gold', 'membership_Silver', 'target_label']\n",
      "   id_column  transaction_amt    temp_c    salary  sensor_reading  user_age  \\\n",
      "0        258         0.308354  0.654032 -0.173041       -1.625937  1.069728   \n",
      "1        964        -0.413340  0.162681 -0.141503        1.685636  0.229410   \n",
      "2        384        -1.213497 -1.478053 -0.081453       -1.364913  1.673729   \n",
      "3        554        -0.614867  0.952677  2.480451        0.443091  1.902042   \n",
      "4        276         1.075554  0.353483 -0.492725        1.546659 -1.763643   \n",
      "\n",
      "   zip_code  join_date_year  join_date_month  join_date_day  ...  \\\n",
      "0  0.618260       -1.144818         0.836908      -0.098094  ...   \n",
      "1 -0.870441        1.404831         0.565008       0.728008  ...   \n",
      "2  0.736693        0.125261        -1.554360       0.380985  ...   \n",
      "3  0.178130        0.125261         0.249976      -0.885129  ...   \n",
      "4  1.025525       -1.128487         1.148619      -1.462992  ...   \n",
      "\n",
      "   city_New York   city_Sf  referral_Billboard  referral_Direct  \\\n",
      "0       1.633324 -0.636785           -0.111080         2.073199   \n",
      "1      -0.618435  1.603304           -0.076162        -0.493369   \n",
      "2      -0.615882  1.588782           -0.070888         2.073165   \n",
      "3       1.623688 -0.629413           -0.070888        -0.482354   \n",
      "4       1.606177 -0.623140           -0.106193         2.066767   \n",
      "\n",
      "   referral_Friend  referral_Google  membership_Bronze  membership_Gold  \\\n",
      "0        -0.070888        -2.012946          -0.706408        -0.701236   \n",
      "1        -0.070888         0.524733          -0.718349        -0.714282   \n",
      "2        -0.070888        -2.007846           1.408927        -0.723747   \n",
      "3        -0.070888         0.498046          -0.709760        -0.723747   \n",
      "4        -0.070888        -2.013630          -0.696244         1.375295   \n",
      "\n",
      "   membership_Silver  target_label  \n",
      "0           1.465910             1  \n",
      "1           1.456754             1  \n",
      "2          -0.687965             0  \n",
      "3           1.453561             0  \n",
      "4          -0.685737             1  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_pickle('/kaggle/working/train_df.pkl')\n",
    "print(\"Columns:\", new_df.columns.tolist())\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6fc2d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:49:06.698516Z",
     "iopub.status.busy": "2025-12-01T07:49:06.697859Z",
     "iopub.status.idle": "2025-12-01T07:49:06.710507Z",
     "shell.execute_reply": "2025-12-01T07:49:06.709499Z"
    },
    "papermill": {
     "duration": 0.02093,
     "end_time": "2025-12-01T07:49:06.711904",
     "exception": false,
     "start_time": "2025-12-01T07:49:06.690974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SUCCESS: Found /kaggle/working/train_df.pkl\n",
      "\n",
      "========================================\n",
      "üß™ PIPELINE VALIDATION REPORT\n",
      "========================================\n",
      "\n",
      "1. DATA VOLUME:\n",
      "   - Original: 1005 rows\n",
      "   - Final:    1658 rows\n",
      "   - Dropped:  -653 rows (-65.0% loss)\n",
      "\n",
      "2. NULL CHECK:\n",
      "   - Remaining Nulls: 0 (Must be 0)\n",
      "\n",
      "3. DIMENSIONS:\n",
      "   - Orig Cols:  14\n",
      "   - Final Cols: 24\n"
     ]
    }
   ],
   "source": [
    "# Time to compare\n",
    "\n",
    "def print_comparison_report(raw, train, test):\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"üß™ PIPELINE VALIDATION REPORT\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # 1. SHAPE & DROPS\n",
    "    total_final = len(train) + len(test)\n",
    "    dropped = len(raw) - total_final\n",
    "    print(f\"\\n1. DATA VOLUME:\")\n",
    "    print(f\"   - Original: {len(raw)} rows\")\n",
    "    print(f\"   - Final:    {total_final} rows\")\n",
    "    print(f\"   - Dropped:  {dropped} rows ({(dropped/len(raw)):.1%} loss)\")\n",
    "\n",
    "    # 2. NULLS\n",
    "    print(f\"\\n2. NULL CHECK:\")\n",
    "    print(f\"   - Remaining Nulls: {train.isnull().sum().sum()} (Must be 0)\")\n",
    "\n",
    "    # 3. COLUMNS (Encoding check)\n",
    "    print(f\"\\n3. DIMENSIONS:\")\n",
    "    print(f\"   - Orig Cols:  {raw.shape[1]}\")\n",
    "    print(f\"   - Final Cols: {train.shape[1]}\")\n",
    "    \n",
    "    # 4. SKEW (Log Transform Check)\n",
    "    if 'Transaction_Amt' in raw.columns and 'Transaction_Amt' in train.columns:\n",
    "        print(f\"\\n4. SKEW CORRECTION:\")\n",
    "        print(f\"   - Orig Max:  ${raw['Transaction_Amt'].max():,.2f}\")\n",
    "        print(f\"   - Final Max: {train['Transaction_Amt'].max():.4f} (Scaled)\")\n",
    "\n",
    "    # 5. BALANCE CHECK\n",
    "    target_col = 'Target_Label'\n",
    "    if target_col in train.columns:\n",
    "        print(f\"\\n5. CLASS BALANCE ({target_col}):\")\n",
    "        tc = train[target_col].value_counts(normalize=True)\n",
    "        print(f\"   - Train (Balanced): 0: {tc.get(0,0):.2f} | 1: {tc.get(1,0):.2f}\")\n",
    "        \n",
    "        testc = test[target_col].value_counts(normalize=True)\n",
    "        print(f\"   - Test (Natural):   0: {testc.get(0,0):.2f} | 1: {testc.get(1,0):.2f}\")\n",
    "\n",
    "train_abs_path = '/kaggle/working/train_df.pkl'\n",
    "test_abs_path = '/kaggle/working/test_df.pkl'\n",
    "\n",
    "if os.path.exists(train_abs_path):\n",
    "    print(f\"\\n‚úÖ SUCCESS: Found {train_abs_path}\")\n",
    "    final_train = pd.read_pickle(train_abs_path)\n",
    "    final_test = pd.read_pickle(test_abs_path)\n",
    "    print_comparison_report(df_raw_snapshot, final_train, final_test)\n",
    "else:\n",
    "    print(f\"\\n‚ùå FAILURE: File still missing at {train_abs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5943ee69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:49:06.725955Z",
     "iopub.status.busy": "2025-12-01T07:49:06.725665Z",
     "iopub.status.idle": "2025-12-01T07:49:06.749653Z",
     "shell.execute_reply": "2025-12-01T07:49:06.748713Z"
    },
    "papermill": {
     "duration": 0.032933,
     "end_time": "2025-12-01T07:49:06.751270",
     "exception": false,
     "start_time": "2025-12-01T07:49:06.718337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üî¨ DEEP DIVE: TRANSFORMATION AUDIT\n",
      "============================================================\n",
      "\n",
      "1. üßπ STANDARDIZATION (Text Cleaning)\n",
      "------------------------------\n",
      "üî¥ Raw Cities:   ['SF' 'New York' 'chicago' '  Austin' 'sf ']\n",
      "üü¢ Clean Cities: Column Dropped/Encoded\n",
      "\n",
      "2. üìÖ DATE ENGINEERING\n",
      "------------------------------\n",
      "üî¥ Raw Date (Row 0): 'Not a Date'\n",
      "üü¢ Extracted Features (Row 0):\n",
      " join_date_year  join_date_month  join_date_day  join_date_dow  join_date_is_weekend\n",
      "      -1.144818         0.836908      -0.098094      -0.994789             -0.625695\n",
      "\n",
      "3. ‚úÇÔ∏è FEATURE SELECTION (Correlation)\n",
      "------------------------------\n",
      "‚úÖ SUCCESS: 'temp_c' kept, 'temp_f' DROPPED (Correlation > 0.95 detected).\n",
      "\n",
      "4. ‚öñÔ∏è SCALING & SKEW (Transaction_Amt)\n",
      "------------------------------\n",
      "STAT       | RAW (Skewed)    | FINAL (Log+Scaled)\n",
      "---------------------------------------------\n",
      "Mean       | 12.23           | 0.0552          (Should be ~0)\n",
      "Std        | 21.61           | 1.0628          (Should be ~1)\n",
      "Max        | 449.75          | 4.6213         \n",
      "\n",
      "5. üî† ENCODING\n",
      "------------------------------\n",
      "‚úÖ One-Hot Detected. 'Membership' expanded to: ['membership_Bronze', 'membership_Gold', 'membership_Silver']\n",
      "   membership_Bronze  membership_Gold  membership_Silver\n",
      "0          -0.706408        -0.701236           1.465910\n",
      "1          -0.718349        -0.714282           1.456754\n",
      "2           1.408927        -0.723747          -0.687965\n",
      "\n",
      "6. ‚öñÔ∏è CLASS IMBALANCE (Oversampling)\n",
      "------------------------------\n",
      "üî¥ Original Ratio: 1s are 8.9% of data\n",
      "üü¢ Training Ratio: 1s are 50.0% of data (Target ~50%)\n",
      "   (Synthetic samples added: 453)\n"
     ]
    }
   ],
   "source": [
    "def deep_dive_report(raw, train):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ DEEP DIVE: TRANSFORMATION AUDIT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. STANDARDIZATION (Text Cleaning)\n",
    "    # Compare raw 'City' to processed 'city'\n",
    "    print(\"\\n1. üßπ STANDARDIZATION (Text Cleaning)\")\n",
    "    print(\"-\" * 30)\n",
    "    try:\n",
    "        # Get top 5 unique values to show consolidation\n",
    "        raw_cities = raw['City'].unique()[:5]\n",
    "        clean_cities = train['city'].unique()[:5] if 'city' in train.columns else \"Column Dropped/Encoded\"\n",
    "        print(f\"üî¥ Raw Cities:   {raw_cities}\")\n",
    "        print(f\"üü¢ Clean Cities: {clean_cities}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compare cities: {e}\")\n",
    "\n",
    "    # 2. DATE ENGINEERING\n",
    "    # Show how 'Join_Date' exploded into features\n",
    "    print(\"\\n2. üìÖ DATE ENGINEERING\")\n",
    "    print(\"-\" * 30)\n",
    "    if 'Join_Date' in raw.columns:\n",
    "        print(f\"üî¥ Raw Date (Row 0): '{raw['Join_Date'].iloc[0]}'\")\n",
    "        \n",
    "        # Find new columns starting with 'join_date'\n",
    "        date_cols = [c for c in train.columns if 'join_date' in c]\n",
    "        if date_cols:\n",
    "            print(f\"üü¢ Extracted Features (Row 0):\")\n",
    "            print(train[date_cols].iloc[0].to_frame().T.to_string(index=False))\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No date features found.\")\n",
    "\n",
    "    # 3. FEATURE SELECTION (Correlation)\n",
    "    # Check if Temp_F (redundant) is gone but Temp_C remains\n",
    "    print(\"\\n3. ‚úÇÔ∏è FEATURE SELECTION (Correlation)\")\n",
    "    print(\"-\" * 30)\n",
    "    has_temp_c = 'temp_c' in train.columns\n",
    "    has_temp_f = 'temp_f' in train.columns\n",
    "    \n",
    "    if has_temp_c and not has_temp_f:\n",
    "        print(\"‚úÖ SUCCESS: 'temp_c' kept, 'temp_f' DROPPED (Correlation > 0.95 detected).\")\n",
    "    elif has_temp_f:\n",
    "        print(\"‚ö†Ô∏è CHECK: 'temp_f' still exists.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Note: Temp columns not found (maybe renamed?).\")\n",
    "\n",
    "    # 4. SCALING (Z-Score)\n",
    "    # Compare distributions of Transaction Amount\n",
    "    print(\"\\n4. ‚öñÔ∏è SCALING & SKEW (Transaction_Amt)\")\n",
    "    print(\"-\" * 30)\n",
    "    if 'Transaction_Amt' in raw.columns and 'transaction_amt' in train.columns:\n",
    "        raw_stats = raw['Transaction_Amt'].describe()\n",
    "        new_stats = train['transaction_amt'].describe()\n",
    "        \n",
    "        print(f\"{'STAT':<10} | {'RAW (Skewed)':<15} | {'FINAL (Log+Scaled)':<15}\")\n",
    "        print(\"-\" * 45)\n",
    "        print(f\"{'Mean':<10} | {raw_stats['mean']:<15.2f} | {new_stats['mean']:<15.4f} (Should be ~0)\")\n",
    "        print(f\"{'Std':<10} | {raw_stats['std']:<15.2f} | {new_stats['std']:<15.4f} (Should be ~1)\")\n",
    "        print(f\"{'Max':<10} | {raw_stats['max']:<15.2f} | {new_stats['max']:<15.4f}\")\n",
    "\n",
    "    # 5. ENCODING\n",
    "    # Show how Membership (Gold/Silver) became numbers or One-Hot\n",
    "    print(\"\\n5. üî† ENCODING\")\n",
    "    print(\"-\" * 30)\n",
    "    # Check for One-Hot columns\n",
    "    membership_cols = [c for c in train.columns if 'membership' in c]\n",
    "    if len(membership_cols) > 1:\n",
    "        print(f\"‚úÖ One-Hot Detected. 'Membership' expanded to: {membership_cols}\")\n",
    "        print(train[membership_cols].head(3))\n",
    "    elif len(membership_cols) == 1:\n",
    "         print(f\"‚úÖ Label Encoding Detected. 'Membership' is now numeric.\")\n",
    "         print(train[membership_cols].head(3))\n",
    "\n",
    "    # 6. BALANCING\n",
    "    print(\"\\n6. ‚öñÔ∏è CLASS IMBALANCE (Oversampling)\")\n",
    "    print(\"-\" * 30)\n",
    "    orig_counts = raw['Target_Label'].value_counts()\n",
    "    new_counts = train['target_label'].value_counts()\n",
    "    \n",
    "    print(f\"üî¥ Original Ratio: 1s are {orig_counts[1] / len(raw):.1%} of data\")\n",
    "    print(f\"üü¢ Training Ratio: 1s are {new_counts[1] / len(train):.1%} of data (Target ~50%)\")\n",
    "    print(f\"   (Synthetic samples added: {len(train) - len(raw)})\")\n",
    "\n",
    "# RUN IT\n",
    "if os.path.exists('/kaggle/working/train_df.pkl'):\n",
    "    final_train = pd.read_pickle('/kaggle/working/train_df.pkl')\n",
    "    # Use df_raw_snapshot from the very beginning of your script\n",
    "    deep_dive_report(df_raw_snapshot, final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d298d1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:49:06.764487Z",
     "iopub.status.busy": "2025-12-01T07:49:06.764181Z",
     "iopub.status.idle": "2025-12-01T07:49:06.768029Z",
     "shell.execute_reply": "2025-12-01T07:49:06.767301Z"
    },
    "papermill": {
     "duration": 0.012154,
     "end_time": "2025-12-01T07:49:06.769343",
     "exception": false,
     "start_time": "2025-12-01T07:49:06.757189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I am just a recent CS grad, so the solutions provided here are a far cry from perfect,\n",
    "# but if there are any questions, please contact me at https://www.linkedin.com/in/yulin-lin-0a05201ab/."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 201.101302,
   "end_time": "2025-12-01T07:49:09.788980",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T07:45:48.687678",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
